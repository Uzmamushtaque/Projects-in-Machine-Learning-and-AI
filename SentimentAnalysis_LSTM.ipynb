{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/Projects-in-Machine-Learning-and-AI/blob/main/SentimentAnalysis_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4657115b",
      "metadata": {
        "id": "4657115b"
      },
      "source": [
        "# **Many-to-One LSTM for Sentiment Analysis and Text Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f2378f",
      "metadata": {
        "id": "05f2378f"
      },
      "source": [
        "## **Project Overview**\n",
        "\n",
        "In this project, we will explore the fascinating world of sentiment analysis and text generation using many-to-one Long Short-Term Memory (LSTM) neural networks.\n",
        "\n",
        "Sentiment Detection:\n",
        "We will dive into sentiment analysis by training an LSTM model to analyze airline sentiments. The dataset consists of sentiment labels (0 or 1) and corresponding text reviews. We will preprocess the data, converting it into a numerical representation using the bag-of-words technique. Through the many-to-one LSTM architecture, we will predict sentiment labels based on the textual input.\n",
        "\n",
        "Text Generation:\n",
        "Next, we will venture into the realm of text generation. Using the beloved classic \"Alice's Adventures in Wonderland\" as our training text, we will prepare and structure the data to facilitate the training of many-to-one LSTMs. These LSTMs will learn the patterns and sequences in the text, enabling us to generate new text based on a given prompt. We will focus specifically on next-word prediction, allowing us to generate coherent and contextually relevant sentences.\n",
        "\n",
        "During our exploration, we will address the challenges of text generation, including the dynamic nature of language and the presence of multiple words with similar meanings. To overcome these challenges, we will utilize techniques such as entropy scaling and softmax temperature. By adjusting the temperature parameter, we can control the randomness and diversity of the generated text.\n",
        "\n",
        "By the end of this project, you will have gained hands-on experience in sentiment analysis and text generation using many-to-one LSTMs. You will understand the intricacies of sentiment detection, as well as the nuances of generating text with contextual relevance. These skills will empower you to apply LSTM-based techniques in various domains, opening up possibilities for analyzing sentiments and generating creative text.\n",
        "\n",
        "![image.png](https://images.pexels.com/photos/693859/pexels-photo-693859.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15dec5f7",
      "metadata": {
        "id": "15dec5f7"
      },
      "source": [
        "## **Learning Outcomes**\n",
        "\n",
        "* Gain insights into sentiment analysis and its importance in analyzing sentiments from textual data.\n",
        "* Learn essential preprocessing techniques for cleaning and preparing textual data for sentiment analysis.\n",
        "* Understand how to convert text into a numerical representation using the bag-of-words technique.\n",
        "* Implement LSTM architecture for the sentiment detection task.\n",
        "* Train and evaluate LSTM models using labeled sentiment data.\n",
        "* Explore text generation using many-to-one LSTMs with \"Alice's Adventures in Wonderland\" as training text.\n",
        "* Understand the challenges of text generation, including language variability and context dependence.\n",
        "* Discover entropy scaling for controlled randomness and diversity in text generation.\n",
        "* Learn about softmax temperature as a hyperparameter to control prediction randomness.\n",
        "* Develop skills in next-word prediction and generating coherent and contextually relevant sentences.\n",
        "* Use this project as a foundation for further exploration in sentiment analysis, text generation, and deep learning with LSTMs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a603d7d",
      "metadata": {
        "id": "1a603d7d"
      },
      "source": [
        "## **Who is this notebook intended for?**\n",
        "\n",
        "This notebook is designed for individuals with a basic understanding of Python and machine learning who are interested in sentiment analysis and text generation using LSTM neural networks. Whether you are a beginner or an experienced practitioner, this notebook provides step-by-step instructions, code snippets, and explanations to guide you through the process. By the end of this notebook, you will have gained the necessary skills to apply LSTM-based techniques in various domains, empowering you to analyze sentiments and generate contextually relevant text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dc5caf8",
      "metadata": {
        "id": "9dc5caf8"
      },
      "source": [
        "#### **We highly recommend watching the project videos for better understanding.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b5a152",
      "metadata": {
        "id": "19b5a152"
      },
      "source": [
        "## **Execution Instructions**\n",
        "<br>\n",
        "\n",
        "### Option 1: Running on your computer locally\n",
        "\n",
        "To run the notebook on your local system set up a [python](https://www.python.org/) environment. Set up the [jupyter notebook](https://jupyter.org/install) with python or by using [anaconda distribution](https://anaconda.org/anaconda/jupyter). Download the notebook and open a jupyter notebook to run the code on local system.\n",
        "\n",
        "The notebook can also be executed by using [Visual Studio Code](https://code.visualstudio.com/), and [PyCharm](https://www.jetbrains.com/pycharm/).\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "### Option 2: Executing with Colab\n",
        "Colab, or \"Collaboratory\", allows you to write and execute Python in your browser, with access to GPUs free of charge and easy sharing.\n",
        "\n",
        "You can run the code using [Google Colab](https://colab.research.google.com/) by uploading the ipython notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e766e1f9",
      "metadata": {
        "id": "e766e1f9"
      },
      "source": [
        "## **Approach**\n",
        "\n",
        "* Sentiment Analysis:\n",
        "\n",
        "    * Dataset:\n",
        "        * Obtain the airline sentiment dataset consisting of sentiment labels (0 or 1) and corresponding text reviews.\n",
        "\n",
        "    * Preprocessing:\n",
        "        * Perform data preprocessing tasks, including text cleaning, tokenization, and removing stop words.\n",
        "        * Convert the text reviews into a bag-of-words representation.\n",
        "\n",
        "    * Many-to-One LSTM:\n",
        "        * Utilize many-to-one LSTM architecture to train the sentiment detection model.\n",
        "        * Feed the bag-of-words representation of the text reviews as input to the LSTM.\n",
        "\n",
        "    * Training:\n",
        "        * Split the dataset into training and testing sets.\n",
        "        * Train the LSTM model using the training set.\n",
        "        * Evaluate the model's performance on the testing set.\n",
        "\n",
        "* Text Generation:\n",
        "\n",
        "    * Dataset:\n",
        "        * Obtain the \"Alice's Adventures in Wonderland\" text dataset.\n",
        "\n",
        "    * Preparing and Structuring:\n",
        "        * Preprocess the text data by cleaning, tokenizing, and structuring the sentences and phrases.\n",
        "        * Create sequences\n",
        "\n",
        "    * Many-to-One LSTM:\n",
        "        * Implement many-to-one LSTM architecture for text generation.\n",
        "        * Train the LSTM model using the prepared dataset.\n",
        "\n",
        "    * The Problem with Text Generation:\n",
        "        * Understand the challenges associated with text generation, such as the variability of language and the dependence on context, style, and word choice.\n",
        "        * Recognize that natural language utilizes a wide variety of words, which may have similar meanings and require careful consideration during generation.\n",
        "\n",
        "    * Randomness through Entropy Scaling:\n",
        "        * Explore the concept of entropy scaling to introduce controlled randomness into text generation.\n",
        "\n",
        "    * Softmax Temperature:\n",
        "        * Introduce the concept of softmax temperature, a hyperparameter used to control the randomness of predictions in LSTMs and neural networks.\n",
        "        * Predicting using the Temperature\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1896a96",
      "metadata": {
        "id": "b1896a96"
      },
      "source": [
        "## **Important Libraries**\n",
        "\n",
        "* **TensorFlow**: TensorFlow is a popular open-source library for machine learning and deep learning. It provides a flexible framework for building and training various types of neural networks. Refer to the [TensorFlow documentation](https://www.tensorflow.org/) for more information.\n",
        "\n",
        "* **NumPy**: NumPy is a fundamental library for scientific computing in Python. It provides support for large, multi-dimensional arrays and a collection of mathematical functions to manipulate and analyze the data efficiently. Refer to the [NumPy documentation](https://numpy.org/) for more information.\n",
        "\n",
        "* **Scikit-learn**: Scikit-learn is a comprehensive machine learning library in Python. It offers various algorithms for classification, regression, clustering, and dimensionality reduction, along with utilities for model evaluation and preprocessing. Refer to the [scikit-learn documentation](https://scikit-learn.org/stable/) for more information.\n",
        "\n",
        "* **Seaborn**: Seaborn is a data visualization library based on Matplotlib. It provides a high-level interface for creating attractive and informative statistical graphics. Refer to the [Seaborn documentation](https://seaborn.pydata.org/) for more information.\n",
        "\n",
        "* **Matplotlib**: Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Refer to the [Matplotlib documentation](https://matplotlib.org/) for more information.\n",
        "\n",
        "* **Pandas**: Pandas is a powerful library for data manipulation and analysis in Python. Refer to the [Pandas documentation](https://pandas.pydata.org/) for more information.\n",
        "\n",
        "* **NLTK**: NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum. Refer to the [NLTK documentation](https://www.nltk.org/) for more information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbbdc20e",
      "metadata": {
        "id": "dbbdc20e"
      },
      "source": [
        "## **Install Packages**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7906baee",
      "metadata": {
        "id": "7906baee"
      },
      "outputs": [],
      "source": [
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e2ba5fa-0e47-4d2d-bb8a-3e5ae32b3935",
      "metadata": {
        "tags": [],
        "id": "2e2ba5fa-0e47-4d2d-bb8a-3e5ae32b3935"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow\n",
        "# !pip install numpy\n",
        "# !pip install pandas\n",
        "# !pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88470f0",
      "metadata": {
        "id": "b88470f0"
      },
      "source": [
        "### **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84fc61e6-2e58-4038-b7d4-904a796a1062",
      "metadata": {
        "tags": [],
        "id": "84fc61e6-2e58-4038-b7d4-904a796a1062"
      },
      "outputs": [],
      "source": [
        "# import relevant packages\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import SimpleRNN, LSTM, GRU\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical, pad_sequences\n",
        "from keras.layers import Embedding\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from projectpro import save_point, checkpoint\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53d9b4a8-9643-4fc1-a87d-4814b446d231",
      "metadata": {
        "tags": [],
        "id": "53d9b4a8-9643-4fc1-a87d-4814b446d231",
        "outputId": "d1dcbcd8-a288-4ae5-b749-0f6c1199e005"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\vithi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "tf.keras.backend.set_image_data_format(\"channels_last\")\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0909a15f",
      "metadata": {
        "id": "0909a15f"
      },
      "source": [
        "## **Refresher: Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fb86619",
      "metadata": {
        "id": "0fb86619"
      },
      "source": [
        "### **Neural Network Architecture**\n",
        "\n",
        "Neural networks, also known as artificial neural networks or simply neural nets, are computational models inspired by the structure and functionality of the human brain. They are widely used in machine learning and deep learning for solving complex problems across various domains.\n",
        "\n",
        "#### **Basic Structure**\n",
        "\n",
        "A neural network consists of interconnected layers of artificial neurons, also known as nodes or units. The layers are organized into an input layer, one or more hidden layers, and an output layer. The input layer receives the input data, the hidden layers process the information, and the output layer produces the final predictions or outputs.\n",
        "\n",
        "#### **Neurons and Connections**\n",
        "\n",
        "Each neuron in a neural network performs a computation on its inputs and produces an output. The neurons in one layer are connected to the neurons in the subsequent layer through weighted connections. These weights determine the strength of the connections and are adjusted during the training process to optimize the network's performance.\n",
        "\n",
        "\n",
        "#### **Mathematics of a Basic Neural Network**\n",
        "\n",
        "A basic neural network consists of multiple layers of neurons connected by weighted connections. Let's consider a neural network with one input layer, one hidden layer, and one output layer.\n",
        "\n",
        "#### **Notation**:\n",
        "\n",
        "- Input layer: $X = [x_1, x_2, ..., x_n]$, where $x_i$ represents the $i$-th input feature.\n",
        "- Hidden layer: $H = [h_1, h_2, ..., h_m]$, where $h_i$ represents the $i$-th neuron in the hidden layer.\n",
        "- Output layer: $Y = [y_1, y_2, ..., y_k]$, where $y_i$ represents the $i$-th output neuron.\n",
        "\n",
        "#### **Forward Propagation**:\n",
        "\n",
        "The weighted sum of inputs for a neuron in the hidden layer is calculated as:\n",
        "\n",
        "$$z_j = \\sum_{i=1}^{n} w_{ji}^{(1)} x_i + b_j^{(1)}$$\n",
        "\n",
        "where $w_{ji}^{(1)}$ represents the weight connecting the $i$-th input to the $j$-th neuron in the hidden layer, and $b_j^{(1)}$ is the bias term for the $j$-th neuron.\n",
        "\n",
        "The output of each neuron in the hidden layer is obtained by applying an activation function $\\sigma$:\n",
        "\n",
        "$$h_j = \\sigma(z_j)$$\n",
        "\n",
        "Similarly, the weighted sum of inputs for a neuron in the output layer is calculated as:\n",
        "\n",
        "$$z_k = \\sum_{j=1}^{m} w_{kj}^{(2)} h_j + b_k^{(2)}$$\n",
        "\n",
        "where $w_{kj}^{(2)}$ represents the weight connecting the $j$-th neuron in the hidden layer to the $k$-th neuron in the output layer, and $b_k^{(2)}$ is the bias term for the $k$-th neuron.\n",
        "\n",
        "The output of each neuron in the output layer is obtained by applying an activation function $\\sigma$:\n",
        "\n",
        "$$y_k = \\sigma(z_k)$$\n",
        "\n",
        "#### **Activation Functions:**\n",
        "\n",
        "Activation functions are a vital component of neural networks. They introduce non-linearity and enable the neural network to learn complex relationships in the data. Here are a few reasons why activation functions are necessary:\n",
        "\n",
        "1. **Non-Linearity**: Without activation functions, the neural network would only be able to approximate linear functions, limiting its learning capacity. Activation functions allow the network to model non-linear relationships between inputs and outputs.\n",
        "\n",
        "2. **Normalization**: Activation functions can normalize the output of a neuron, ensuring that the values fall within a desired range. This can help in stabilizing the learning process and improving the convergence of the network.\n",
        "\n",
        "3. **Differentiability**: Activation functions are differentiable, which is essential for training neural networks using gradient-based optimization algorithms like backpropagation. The gradients of the activation functions help in updating the weights and biases during the training process.\n",
        "\n",
        "\n",
        "\n",
        "Common activation functions used in neural networks include the sigmoid function, tanh (hyperbolic tangent) function, and Rectified Linear Unit (ReLU) function.\n",
        "\n",
        "By applying activation functions after the weighted sum of inputs, neural networks can model complex relationships and make non-linear predictions, enabling them to solve a wide range of problems.\n",
        "\n",
        "\n",
        "#### **Feedforward Propagation**\n",
        "\n",
        "In feedforward propagation, the information flows through the network in the forward direction, starting from the input layer and passing through the hidden layers until it reaches the output layer. Each neuron receives inputs from the previous layer, computes its weighted sum, applies the activation function, and passes the output to the next layer.\n",
        "\n",
        "#### **Backpropagation**\n",
        "\n",
        "Backpropagation is an algorithm used to train neural networks by adjusting the weights based on the calculated gradients of the loss function with respect to the weights. It involves computing the error between the predicted outputs and the actual targets and propagating this error backward through the network to update the weights.\n",
        "\n",
        "#### **Loss Functions**\n",
        "\n",
        "Loss functions quantify the difference between the predicted outputs of the neural network and the actual targets. Common loss functions include mean squared error (MSE) for regression tasks and cross-entropy for classification tasks. The choice of the loss function depends on the nature of the problem being solved.\n",
        "\n",
        "#### **Optimization Algorithms**\n",
        "\n",
        "Optimization algorithms, such as stochastic gradient descent (SGD) and its variants (e.g., Adam, RMSprop), are used to minimize the loss function and update the weights of the neural network during training. These algorithms adjust the weights iteratively based on the gradients computed through backpropagation.\n",
        "\n",
        "#### **Hidden Layers and Network Depth**\n",
        "\n",
        "The hidden layers in a neural network perform the computations necessary for feature extraction and representation learning. The number of hidden layers and the number of neurons in each layer, referred to as the network's depth, are hyperparameters that can be adjusted based on the complexity of the problem and the available computational resources.\n",
        "\n",
        "#### **Deep Neural Networks**\n",
        "\n",
        "Deep neural networks refer to neural networks with multiple hidden layers. Deep learning has gained significant attention due to the ability of deep neural networks to learn hierarchical representations and solve complex problems in areas such as computer vision, natural language processing, and speech recognition.\n",
        "\n",
        "Neural network architecture plays a crucial role in the performance and capabilities of the model. Choosing the right architecture, including the number of layers, the number of neurons, and the activation functions, is essential for achieving optimal results in various machine learning and deep learning tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06d519e4",
      "metadata": {
        "id": "06d519e4"
      },
      "source": [
        "### **Neural Network Prediction for Regression and Classification**\n",
        "\n",
        "Neural networks are versatile models that can be used for both regression and classification tasks. The prediction process differs slightly depending on the type of problem being addressed.\n",
        "\n",
        "#### **Regression Prediction**\n",
        "\n",
        "In regression tasks, the goal is to predict a continuous numerical value as the output. Here's how neural networks make predictions for regression:\n",
        "\n",
        "1. **Feedforward Propagation**: The input data is passed through the neural network in the forward direction. Each neuron in the network receives inputs from the previous layer, computes the weighted sum of inputs, applies an activation function, and passes the output to the next layer. This process continues until the output layer is reached.\n",
        "\n",
        "2. **Output Layer**: In regression, the output layer typically consists of a single neuron that produces a continuous numerical value. The activation function used in the output layer depends on the specific requirements of the problem. For example, a ReLU function is commonly used for regression tasks.\n",
        "\n",
        "3. **Final Prediction**: The output value of the neural network's output neuron represents the predicted value for the regression task. It can be interpreted as the model's estimation or approximation of the target value based on the given input.\n",
        "\n",
        "#### **Classification Prediction**\n",
        "\n",
        "In classification tasks, the goal is to assign input data to specific categories or classes. Neural networks can perform multi-class or binary classification. Here's how neural networks make predictions for classification:\n",
        "\n",
        "1. **Feedforward Propagation**: Similar to regression, the input data is propagated through the network in the forward direction. Each neuron computes the weighted sum of inputs, applies an activation function, and passes the output to the next layer.\n",
        "\n",
        "2. **Output Layer**: In classification, the output layer depends on the number of classes in the problem. For binary classification, the output layer typically consists of a single neuron using a sigmoid activation function, which produces a value between 0 and 1 representing the probability of belonging to the positive class. For multi-class classification, the output layer may consist of multiple neurons using softmax activation, where each neuron represents the probability of belonging to a specific class.\n",
        "\n",
        "3. **Final Prediction**: In binary classification, the predicted class can be determined based on a threshold value (e.g., 0.5). If the output probability is above the threshold, the sample is classified as the positive class; otherwise, it is classified as the negative class. In multi-class classification, the class with the highest predicted probability is assigned as the predicted class.\n",
        "\n",
        "Neural networks learn the mapping between the input data and the desired output through the training process, where the weights and biases are adjusted to minimize the error between the predicted output and the actual target values. Once trained, the neural network can be used to make predictions on new, unseen data.\n",
        "\n",
        "Understanding how neural networks make predictions in regression and classification tasks is crucial for interpreting the model's outputs and evaluating its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ef62721",
      "metadata": {
        "id": "3ef62721"
      },
      "source": [
        "### **Recurrent Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27b01094",
      "metadata": {
        "id": "27b01094"
      },
      "source": [
        "RNN were created because there were a few issues in the feed-forward neural network:\n",
        "\n",
        "Cannot handle sequential data\n",
        "Considers only the current input\n",
        "Cannot memorize previous inputs\n",
        "The solution to these issues is the RNN. An RNN can handle sequential data, accepting the current input data, and previously received inputs. RNNs can memorize previous inputs due to their internal memory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d15ec7e",
      "metadata": {
        "id": "4d15ec7e"
      },
      "source": [
        "RNN works on the principle of saving the output of a particular layer and feeding this back to the input in order to predict the output of the layer.\n",
        "\n",
        "Below is how you can convert a Feed-Forward Neural Network into a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5939692b",
      "metadata": {
        "id": "5939692b"
      },
      "source": [
        "<img src=\"images/rnn.png\"\n",
        "     align=\"center\"\n",
        "     width=\"700\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4e7dbff",
      "metadata": {
        "id": "e4e7dbff"
      },
      "source": [
        "The nodes in different layers of the neural network are compressed to form a single layer of recurrent neural networks. A, B, and C are the parameters of the network."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bec2671c",
      "metadata": {
        "id": "bec2671c"
      },
      "source": [
        "<img src=\"./images/rnn_animation.gif\"\n",
        "     align=\"center\"\n",
        "     width=\"450\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582a4990",
      "metadata": {
        "id": "582a4990"
      },
      "source": [
        "The four commonly used types of Recurrent Neural Networks are:\n",
        "\n",
        "**One-to-One**: The simplest type of RNN is One-to-One, which allows a single input and a single output. It has fixed input and output sizes and acts as a traditional neural network. The One-to-One application can be found in Image Classification.\n",
        "\n",
        "**One-to-Many**: One-to-Many is a type of RNN that gives multiple outputs when given a single input. It takes a fixed input size and gives a sequence of data outputs. Its applications can be found in Music Generation and Image Captioning.\n",
        "\n",
        "**Many-to-One**: Many-to-One is used when a single output is required from multiple input units or a sequence of them. It takes a sequence of inputs to display a fixed output. Sentiment Analysis is a common example of this type of Recurrent Neural Network.\n",
        "\n",
        "**Many-to-Many**: Many-to-Many is used to generate a sequence of output data from a sequence of input units.\n",
        "\n",
        "This type of RNN is further divided into ﻿the following two subcategories:\n",
        "\n",
        "    1. Equal Unit Size: In this case, the number of both the input and output units is the same. A common application can be found in Name-Entity Recognition.\n",
        "    2. Unequal Unit Size: In this case, inputs and outputs have different numbers of units. Its application can be found in Machine Translation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46a8d520",
      "metadata": {
        "id": "46a8d520"
      },
      "source": [
        "<img src=\"./images/types_rnn.png\"\n",
        "     align=\"center\"\n",
        "     width=\"750\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94067480",
      "metadata": {
        "id": "94067480"
      },
      "source": [
        "### **LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1202225c",
      "metadata": {
        "id": "1202225c"
      },
      "source": [
        "Now, even though RNNs are quite powerful, they suffer from Vanishing gradient problem which hinders them from using long term information, like they are good for storing memory 3-4 instances of past iterations but larger number of instances don't provide good results so we don't just use regular RNNs. Instead, we use a better variation of RNNs: Long Short Term Networks(LSTM)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57df53f1",
      "metadata": {
        "id": "57df53f1"
      },
      "source": [
        "**What is Vanishing Gradient problem?**\n",
        "\n",
        "Vanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional activation functions such as the hyperbolic tangent function have gradients in the range (0, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the \"front\" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n while the front layers train very slowly.\n",
        "\n",
        "<img src=\"./images/decay.png\"\n",
        "     align=\"center\"\n",
        "     width=\"750\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f5b40d",
      "metadata": {
        "id": "67f5b40d"
      },
      "source": [
        "**Fixing the Vanishing/Exploding Gradient with LSTMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1dc68dd",
      "metadata": {
        "id": "e1dc68dd"
      },
      "source": [
        "Long short-term memory (LSTM) units (or blocks) are a building unit for layers of a recurrent neural network (RNN). A RNN composed of LSTM units is often called an LSTM network. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell is responsible for \"remembering\" values over arbitrary time intervals; hence the word \"memory\" in LSTM. Each of the three gates can be thought of as a \"conventional\" artificial neuron, as in a multi-layer (or feedforward) neural network: that is, they compute an activation (using an activation function) of a weighted sum. Intuitively, they can be thought as regulators of the flow of values that goes through the connections of the LSTM; hence the denotation \"gate\". There are connections between these gates and the cell.\n",
        "\n",
        "The expression long short-term refers to the fact that LSTM is a model for the short-term memory which can last for a long period of time. An LSTM is well-suited to classify, process and predict time series given time lags of unknown size and duration between important events. LSTMs were developed to deal with the exploding and vanishing gradient problem when training traditional RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12af99c9",
      "metadata": {
        "id": "12af99c9"
      },
      "source": [
        "<img src=\"./images/lstm.png\"\n",
        "     align=\"center\"\n",
        "     width=\"750\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "114acb76",
      "metadata": {
        "id": "114acb76"
      },
      "source": [
        "More on LSTMs: https://medium.com/deep-math-machine-learning-ai/chapter-10-1-deepnlp-lstm-long-short-term-memory-networks-with-math-21477f8e4235"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cffe06da-9d21-4545-80d3-7f0f0b28f9e2",
      "metadata": {
        "tags": [],
        "id": "cffe06da-9d21-4545-80d3-7f0f0b28f9e2"
      },
      "source": [
        "## **Sentiment Detection with (Many to One) LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55c4aef7-c740-4e22-b3c7-f0c7ad800e6b",
      "metadata": {
        "id": "55c4aef7-c740-4e22-b3c7-f0c7ad800e6b"
      },
      "source": [
        "Previously we looked at one-to-one RNN type, which is the basic structure. And next one is one-to-many type. For example, if the model gets the fixed format like image as an input, it generates the sequence data. You can see the implementation on image caption application. Another type is many-to-many type. It gets sequence data as an inputs, and also generates the sequence data as an output. Common application of many-to-many type is machine translation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a11b510-2b47-4541-ac3f-18d7d9682afe",
      "metadata": {
        "tags": [],
        "id": "8a11b510-2b47-4541-ac3f-18d7d9682afe"
      },
      "source": [
        "<img src=\"./images/many-to-one.png\"\n",
        "     align=\"center\"\n",
        "     width=\"400\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38511973-d6b4-4c85-8804-0fb303e2b24d",
      "metadata": {
        "tags": [],
        "id": "38511973-d6b4-4c85-8804-0fb303e2b24d"
      },
      "source": [
        "Many-to-one type, which is our topic in this post, gets an sequence data as an input and generates some informatic data like labels. So we can use it for classification. Suppose that someone defines the sentiment of each sentence, and train the model with many-to-one type. And when the model gets the unseen sentence, then it will predict the intention of sentence, good or bad."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "820903c3-4b7b-493c-91bd-710551de1030",
      "metadata": {
        "id": "820903c3-4b7b-493c-91bd-710551de1030"
      },
      "source": [
        "Suppose we have a sentence, **\"This movie is interesting\"**. And we want to classify the sentiment of this sentence. In order to do this, we need to apply tokenization in word level. If this sentensce intends the good sentiment, then word token may contains good words, like \"good\". So we can classify this sentence to good sentiment.\n",
        "\n",
        "So if we want to apply it in RNN model, we need to consider the sentence as a word sequence (many), then classify its label (one). That is process of many-to-one type model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5a4e0b3-0340-4880-b799-cbc402c1b47c",
      "metadata": {
        "id": "e5a4e0b3-0340-4880-b799-cbc402c1b47c"
      },
      "source": [
        "<img src=\"./images/many-to-one_detail.png\"\n",
        "     align=\"center\"\n",
        "     width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f38dea48-6cf7-4994-8c5c-d19bd0b659e9",
      "metadata": {
        "id": "f38dea48-6cf7-4994-8c5c-d19bd0b659e9"
      },
      "source": [
        "## **Reading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97bf1eef-09cd-4d5a-b74e-b5bfedac54b0",
      "metadata": {
        "tags": [],
        "id": "97bf1eef-09cd-4d5a-b74e-b5bfedac54b0",
        "outputId": "fa1685d5-4048-4789-c52f-c3bc02441190"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   airline_sentiment  \\\n",
              "0                  1   \n",
              "1                  0   \n",
              "2                  0   \n",
              "3                  0   \n",
              "4                  1   \n",
              "\n",
              "                                                                                                                                       text  \n",
              "0                                                                  @VirginAmerica plus you've added commercials to the experience... tacky.  \n",
              "1            @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse  \n",
              "2                                                                                   @VirginAmerica and it's a really big bad thing about it  \n",
              "3  @VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA  \n",
              "4                                                           @VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('../data/airline_sentiment.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c7e1285-70ca-4c4c-b907-8a0dc2051cb5",
      "metadata": {
        "id": "1c7e1285-70ca-4c4c-b907-8a0dc2051cb5"
      },
      "source": [
        "## **Pre Processing data for training**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baf02e38-3e13-429a-9151-fc8efd3b4120",
      "metadata": {
        "id": "baf02e38-3e13-429a-9151-fc8efd3b4120"
      },
      "source": [
        "- Normalise sentences: remove special characters, convert to lower case etc.\n",
        "- Remove stop words to give more relevance to specific words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa7d052c",
      "metadata": {
        "id": "fa7d052c"
      },
      "source": [
        "**Normalizing Sentences:**\n",
        "\n",
        "Normalizing sentences is an important step in text preprocessing for sentiment analysis and text generation tasks. It involves applying various techniques to standardize the text data and make it more uniform. Here are some reasons why normalizing sentences is done:\n",
        "\n",
        "    * Remove Special Characters: Special characters such as punctuation marks, symbols, or emojis do not contribute much to the sentiment or meaning of a sentence. By removing these special characters, we can focus on the essential words and improve the efficiency of the subsequent analysis steps.\n",
        "\n",
        "    * Convert to Lower Case: Converting all text to lower case helps in reducing the dimensionality of the data. It ensures that the same words appearing in different cases (e.g., \"good\" and \"Good\") are treated as the same word during analysis. This step avoids duplication and helps in capturing the overall sentiment more accurately.\n",
        "\n",
        "    * Uniformity and Consistency: Normalizing sentences ensures that the text data is consistent and follows a standard format. It helps in creating a uniform representation of words and phrases, allowing the subsequent models to learn patterns and relationships effectively.\n",
        "\n",
        "**Removing Stop Words:**\n",
        "\n",
        "Stop words are commonly occurring words in a language that do not carry much meaning or contribute significantly to the sentiment of a sentence. Examples of stop words include \"a,\" \"the,\" \"and,\" \"is,\" and so on. Here are the reasons why removing stop words is done:\n",
        "\n",
        "    * Relevance to Specific Words: Stop words occur frequently in the text but often do not add much value to sentiment analysis or text generation. Removing stop words can help in focusing on the more meaningful and informative words, providing better context for sentiment analysis and generating more relevant and coherent text.\n",
        "\n",
        "    * Noise Reduction: Stop words can introduce noise and unwanted variation in the text data. By eliminating them, we can reduce the noise level and improve the signal-to-noise ratio. This leads to better analysis results and more accurate predictions.\n",
        "\n",
        "    * Reducing Dimensionality: Stop words are typically high-frequency words that appear in almost every sentence. By removing them, we can reduce the dimensionality of the data and improve computational efficiency during model training and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b126e338-4896-43cd-a4c3-2a3d8abce629",
      "metadata": {
        "tags": [],
        "id": "b126e338-4896-43cd-a4c3-2a3d8abce629"
      },
      "outputs": [],
      "source": [
        "# Loading English stop words\n",
        "stop = nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3988683c-09bf-456f-a308-4ef5070dbb23",
      "metadata": {
        "tags": [],
        "id": "3988683c-09bf-456f-a308-4ef5070dbb23"
      },
      "outputs": [],
      "source": [
        "def pre_process_text_data(text: str) -> str:\n",
        "    # normalize and remove special characters\n",
        "    text = text.lower()\n",
        "    text = re.sub('[^0-9a-zA-Z]+',' ',text)\n",
        "    # remove stop words\n",
        "    words = text.split()\n",
        "    words = [w for w in words if (w not in stop)]\n",
        "    words = ' '.join(words)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98fb0090-d947-4e5d-9bcb-935291c1ed56",
      "metadata": {
        "tags": [],
        "id": "98fb0090-d947-4e5d-9bcb-935291c1ed56",
        "outputId": "763bb8ec-dfcf-4d69-f6b4-3d81381295cc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>virginamerica plus added commercials experience tacky</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>virginamerica really aggressive blast obnoxious entertainment guests faces amp little recourse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>virginamerica really big bad thing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>virginamerica seriously would pay 30 flight seats playing really bad thing flying va</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>virginamerica yes nearly every time fly vx ear worm go away</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   airline_sentiment  \\\n",
              "0                  1   \n",
              "1                  0   \n",
              "2                  0   \n",
              "3                  0   \n",
              "4                  1   \n",
              "\n",
              "                                                                                             text  \n",
              "0                                           virginamerica plus added commercials experience tacky  \n",
              "1  virginamerica really aggressive blast obnoxious entertainment guests faces amp little recourse  \n",
              "2                                                              virginamerica really big bad thing  \n",
              "3            virginamerica seriously would pay 30 flight seats playing really bad thing flying va  \n",
              "4                                     virginamerica yes nearly every time fly vx ear worm go away  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text'] = df['text'].apply(pre_process_text_data)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d978232-d848-43f3-a48a-6cac90c18f9a",
      "metadata": {
        "id": "3d978232-d848-43f3-a48a-6cac90c18f9a"
      },
      "source": [
        "## **Checking most used words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14a6004-9fcf-45ea-9623-aff5bd10a3b0",
      "metadata": {
        "tags": [],
        "id": "b14a6004-9fcf-45ea-9623-aff5bd10a3b0"
      },
      "outputs": [],
      "source": [
        "counts = Counter()\n",
        "for i, review in enumerate(df['text']):\n",
        "    counts.update(review.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbd30f06-a179-43cd-b8a5-0aeab410b7e7",
      "metadata": {
        "tags": [],
        "id": "cbd30f06-a179-43cd-b8a5-0aeab410b7e7",
        "outputId": "78adaad0-d55a-40d3-a2e9-d05c0ea0c9a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['united',\n",
              " 'flight',\n",
              " 'usairways',\n",
              " 'americanair',\n",
              " 'southwestair',\n",
              " 'jetblue',\n",
              " 'get',\n",
              " 'cancelled',\n",
              " 'thanks',\n",
              " 'service']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words = sorted(counts, key=counts.get, reverse=True)\n",
        "words[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb92d579-609b-4636-b199-4e1a728e7046",
      "metadata": {
        "id": "eb92d579-609b-4636-b199-4e1a728e7046"
      },
      "source": [
        "## **Creating numeric representation of words (Bag of Words)**\n",
        "\n",
        "The bag-of-words representation is a technique used to convert text data into a numerical representation that can be used for machine learning algorithms. It disregards the order and structure of words in a sentence and focuses solely on the frequency of occurrence of words. Here's an overview of the steps involved in creating a bag-of-words representation:\n",
        "\n",
        "    * Tokenization: The text is split into individual words or tokens. Each token represents a unit of meaning, such as a word or a combination of words.\n",
        "\n",
        "    * Vocabulary Creation: All unique tokens in the text data are collected to create a vocabulary or a dictionary. This vocabulary serves as the set of all possible features for the bag-of-words representation.\n",
        "\n",
        "    * Frequency Count: For each document or sentence in the text data, the frequency of each token in the vocabulary is counted. This creates a numerical representation of the text data, where each entry represents the frequency of a specific token in a particular document.\n",
        "\n",
        "    * Checking Sequence Length and Padding: In some cases, the length of different sentences or documents may vary. To handle this, the sequence length is checked, and padding is applied to ensure that all sequences have the same length.\n",
        "\n",
        "The resulting bag-of-words representation is typically a matrix where each row represents a document or sentence, and each column corresponds to a specific token in the vocabulary. The values in the matrix represent the frequency or occurrence of each token in the corresponding document.\n",
        "\n",
        "Example:\n",
        "\n",
        "| Document  | Word1 | Word2 | Word3 | Word4 | Word5 |\n",
        "|-----------|-------|-------|-------|-------|-------|\n",
        "| Sentence1 | 2     | 1     | 0     | 1     | 0     |\n",
        "| Sentence2 | 0     | 1     | 1     | 0     | 1     |\n",
        "| Sentence3 | 1     | 0     | 0     | 1     | 1     |\n",
        "| Sentence4 | 0     | 1     | 0     | 0     | 0     |\n",
        "\n",
        "In this example, we have four sentences (Sentence1, Sentence2, Sentence3, and Sentence4) with five unique words (Word1, Word2, Word3, Word4, and Word5). The table represents the bag-of-words representation, where each entry indicates the frequency of the corresponding word in the respective sentence. For instance, in Sentence1, Word1 occurs twice, Word2 occurs once, and so on.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ca371c-e4de-4f64-89f5-61108c021809",
      "metadata": {
        "tags": [],
        "id": "d3ca371c-e4de-4f64-89f5-61108c021809"
      },
      "outputs": [],
      "source": [
        "word_to_int = {word: i for i, word in enumerate(words, start=1)}\n",
        "int_to_word = {i: word for i, word in enumerate(words, start=1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd6dea37-2c71-44ce-8402-8c1cc5238ecf",
      "metadata": {
        "tags": [],
        "id": "cd6dea37-2c71-44ce-8402-8c1cc5238ecf"
      },
      "outputs": [],
      "source": [
        "def text_to_int(text:str, word_to_int: dict):\n",
        "    return [word_to_int[word] for word in review.split()]\n",
        "\n",
        "\n",
        "def int_to_text(int_arr, int_to_word: dict):\n",
        "    return ' '.join([int_to_word[index] for index in int_arr if index != 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a158248-7fe8-43c2-9ef6-84f7e10b3e8b",
      "metadata": {
        "tags": [],
        "id": "5a158248-7fe8-43c2-9ef6-84f7e10b3e8b"
      },
      "outputs": [],
      "source": [
        "mapped_reviews = []\n",
        "for review in df['text']:\n",
        "    mapped_reviews.append(text_to_int(review, word_to_int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b3aa841-1724-4750-9c53-87eb2a34a5e3",
      "metadata": {
        "tags": [],
        "id": "4b3aa841-1724-4750-9c53-87eb2a34a5e3",
        "outputId": "3cc175a1-16ad-4844-b4f0-77aef09c19d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: virginamerica plus added commercials experience tacky\n",
            "Mapped text: [44, 450, 1142, 2233, 100, 5429]\n"
          ]
        }
      ],
      "source": [
        "print(f'Original text: {df.loc[0][\"text\"]}')\n",
        "print(f'Mapped text: {mapped_reviews[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eec94556-0a3f-40d9-ac69-6bae85c21bc1",
      "metadata": {
        "tags": [],
        "id": "eec94556-0a3f-40d9-ac69-6bae85c21bc1"
      },
      "outputs": [],
      "source": [
        "length_sent = []\n",
        "for i in range(len(mapped_reviews)):\n",
        "    length_sent.append(len(mapped_reviews[i]))\n",
        "checkpoint('5b420a')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23d75bf5-0024-4998-80b5-471af44be311",
      "metadata": {
        "id": "23d75bf5-0024-4998-80b5-471af44be311"
      },
      "source": [
        "## **Checking sequence length and padding accordingly**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbf03aea-686d-43a5-87a7-4e87acb094f4",
      "metadata": {
        "tags": [],
        "id": "bbf03aea-686d-43a5-87a7-4e87acb094f4"
      },
      "outputs": [],
      "source": [
        "sequence_length = max(length_sent)\n",
        "X = pad_sequences(maxlen = sequence_length,\n",
        "                  sequences = mapped_reviews,\n",
        "                  padding = \"post\",\n",
        "                  value = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "918a9dc5-55c7-4edc-b8d0-37fb34aafa53",
      "metadata": {
        "tags": [],
        "id": "918a9dc5-55c7-4edc-b8d0-37fb34aafa53",
        "outputId": "d820a4b1-6a96-496e-8d4c-24b0e84eaaf1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  44,  450, 1142, 2233,  100, 5429,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a62fc56-7eb0-4f9d-ac69-76f6bf3e466d",
      "metadata": {
        "tags": [],
        "id": "3a62fc56-7eb0-4f9d-ac69-76f6bf3e466d"
      },
      "outputs": [],
      "source": [
        "y = df['airline_sentiment'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dccdd1fc-aa41-44d4-966e-92e25d20df3f",
      "metadata": {
        "id": "dccdd1fc-aa41-44d4-966e-92e25d20df3f"
      },
      "source": [
        "## **Creating LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a94d87-19a4-49a3-9478-4254fada66e3",
      "metadata": {
        "tags": [],
        "id": "94a94d87-19a4-49a3-9478-4254fada66e3"
      },
      "outputs": [],
      "source": [
        "embedding_vecor_length = 32\n",
        "max_review_length = 26"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b21f4b95-583e-4985-af99-5365223129b3",
      "metadata": {
        "tags": [],
        "id": "b21f4b95-583e-4985-af99-5365223129b3"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=12533, output_dim=32, input_length = 26))\n",
        "model.add(LSTM(40, return_sequences=True))\n",
        "model.add(LSTM(40, return_sequences=False))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ee269b2-3df0-4371-aa7a-a70c225ec7cb",
      "metadata": {
        "tags": [],
        "id": "6ee269b2-3df0-4371-aa7a-a70c225ec7cb",
        "outputId": "4b54d3ec-e094-45c2-8fa4-15c9a2f7adfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 26, 32)            401056    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 26, 40)            11680     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 40)                12960     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 82        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 425,778\n",
            "Trainable params: 425,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c65953e9-6e92-41ad-820b-3b5020245f9f",
      "metadata": {
        "id": "c65953e9-6e92-41ad-820b-3b5020245f9f"
      },
      "source": [
        "## **Preparing data for training and validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9848c86-31b7-4f36-ae64-cc4468f90071",
      "metadata": {
        "tags": [],
        "id": "a9848c86-31b7-4f36-ae64-cc4468f90071"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=10)\n",
        "save_point('5b420a')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c36922c-6f1e-41bd-ae76-30e4136c56ff",
      "metadata": {
        "id": "8c36922c-6f1e-41bd-ae76-30e4136c56ff"
      },
      "source": [
        "## **One-hot-encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50ddca1a-8d90-4f5b-957f-b26cb85f0539",
      "metadata": {
        "tags": [],
        "id": "50ddca1a-8d90-4f5b-957f-b26cb85f0539"
      },
      "outputs": [],
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "220f6659-5c51-4a63-9b58-27d6591271bf",
      "metadata": {
        "id": "220f6659-5c51-4a63-9b58-27d6591271bf"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18a3b18c-aa25-48ff-b59a-4c8a5795216c",
      "metadata": {
        "tags": [],
        "id": "18a3b18c-aa25-48ff-b59a-4c8a5795216c",
        "outputId": "99f98cac-28aa-4c2b-a2da-96c1457d06c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "253/253 [==============================] - 7s 18ms/step - loss: 0.3540 - accuracy: 0.8581 - val_loss: 0.2151 - val_accuracy: 0.9090\n",
            "Epoch 2/50\n",
            "253/253 [==============================] - 4s 16ms/step - loss: 0.1457 - accuracy: 0.9495 - val_loss: 0.2179 - val_accuracy: 0.9194\n",
            "Epoch 3/50\n",
            "253/253 [==============================] - 4s 16ms/step - loss: 0.0763 - accuracy: 0.9752 - val_loss: 0.2963 - val_accuracy: 0.9174\n",
            "Epoch 4/50\n",
            "253/253 [==============================] - 4s 16ms/step - loss: 0.0437 - accuracy: 0.9860 - val_loss: 0.3533 - val_accuracy: 0.8908\n",
            "Epoch 5/50\n",
            "253/253 [==============================] - 4s 18ms/step - loss: 0.0335 - accuracy: 0.9889 - val_loss: 0.3947 - val_accuracy: 0.9116\n",
            "Epoch 6/50\n",
            "253/253 [==============================] - 4s 18ms/step - loss: 0.0234 - accuracy: 0.9936 - val_loss: 0.3262 - val_accuracy: 0.9012\n",
            "Epoch 7/50\n",
            "253/253 [==============================] - 4s 17ms/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.5257 - val_accuracy: 0.9012\n",
            "Epoch 8/50\n",
            "253/253 [==============================] - 4s 17ms/step - loss: 0.0148 - accuracy: 0.9962 - val_loss: 0.3979 - val_accuracy: 0.9012\n",
            "Epoch 9/50\n",
            "253/253 [==============================] - 4s 16ms/step - loss: 0.0135 - accuracy: 0.9967 - val_loss: 0.5139 - val_accuracy: 0.8975\n",
            "Epoch 10/50\n",
            "253/253 [==============================] - 4s 16ms/step - loss: 0.0112 - accuracy: 0.9973 - val_loss: 0.4935 - val_accuracy: 0.8986\n",
            "Epoch 11/50\n",
            "253/253 [==============================] - 4s 16ms/step - loss: 0.0079 - accuracy: 0.9983 - val_loss: 0.5624 - val_accuracy: 0.8952\n",
            "Epoch 12/50\n",
            "253/253 [==============================] - 4s 16ms/step - loss: 0.0088 - accuracy: 0.9981 - val_loss: 0.4811 - val_accuracy: 0.8978\n",
            "Epoch 13/50\n",
            "253/253 [==============================] - 4s 16ms/step - loss: 0.0126 - accuracy: 0.9969 - val_loss: 0.4964 - val_accuracy: 0.9004\n",
            "Epoch 14/50\n",
            "253/253 [==============================] - 4s 16ms/step - loss: 0.0108 - accuracy: 0.9974 - val_loss: 0.4832 - val_accuracy: 0.9047\n",
            "Epoch 15/50\n",
            "253/253 [==============================] - 4s 16ms/step - loss: 0.0076 - accuracy: 0.9983 - val_loss: 0.6435 - val_accuracy: 0.9024\n",
            "Epoch 16/50\n",
            "253/253 [==============================] - 4s 17ms/step - loss: 0.0153 - accuracy: 0.9969 - val_loss: 0.5102 - val_accuracy: 0.9050\n",
            "Epoch 17/50\n",
            "253/253 [==============================] - 5s 18ms/step - loss: 0.0079 - accuracy: 0.9981 - val_loss: 0.6562 - val_accuracy: 0.8989\n",
            "Epoch 18/50\n",
            "253/253 [==============================] - 5s 18ms/step - loss: 0.0133 - accuracy: 0.9968 - val_loss: 0.5780 - val_accuracy: 0.8975\n",
            "Epoch 19/50\n",
            "253/253 [==============================] - 5s 18ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.5678 - val_accuracy: 0.8943\n",
            "Epoch 20/50\n",
            "253/253 [==============================] - 5s 18ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.6605 - val_accuracy: 0.9010\n",
            "Epoch 21/50\n",
            "253/253 [==============================] - 5s 18ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.6309 - val_accuracy: 0.9012\n",
            "Epoch 22/50\n",
            "253/253 [==============================] - 5s 21ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.6379 - val_accuracy: 0.8995\n",
            "Epoch 23/50\n",
            "253/253 [==============================] - 11s 42ms/step - loss: 0.0113 - accuracy: 0.9975 - val_loss: 0.6425 - val_accuracy: 0.8871\n",
            "Epoch 24/50\n",
            "253/253 [==============================] - 13s 52ms/step - loss: 0.0090 - accuracy: 0.9981 - val_loss: 0.5042 - val_accuracy: 0.9027\n",
            "Epoch 25/50\n",
            "253/253 [==============================] - 13s 53ms/step - loss: 0.0075 - accuracy: 0.9983 - val_loss: 0.4995 - val_accuracy: 0.8802\n",
            "Epoch 26/50\n",
            "253/253 [==============================] - 12s 47ms/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 0.6839 - val_accuracy: 0.8975\n",
            "Epoch 27/50\n",
            "253/253 [==============================] - 4s 17ms/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 0.5074 - val_accuracy: 0.8992\n",
            "Epoch 28/50\n",
            "253/253 [==============================] - 4s 16ms/step - loss: 0.0050 - accuracy: 0.9991 - val_loss: 0.5751 - val_accuracy: 0.9004\n",
            "Epoch 29/50\n",
            "253/253 [==============================] - 4s 16ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.6435 - val_accuracy: 0.8943\n",
            "Epoch 30/50\n",
            "253/253 [==============================] - 5s 20ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.6693 - val_accuracy: 0.8949\n",
            "Epoch 31/50\n",
            "253/253 [==============================] - 5s 21ms/step - loss: 0.0062 - accuracy: 0.9989 - val_loss: 0.5200 - val_accuracy: 0.8807\n",
            "Epoch 32/50\n",
            "253/253 [==============================] - 5s 19ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.5985 - val_accuracy: 0.8943\n",
            "Epoch 33/50\n",
            "253/253 [==============================] - 4s 18ms/step - loss: 0.0091 - accuracy: 0.9978 - val_loss: 0.5835 - val_accuracy: 0.8958\n",
            "Epoch 34/50\n",
            "253/253 [==============================] - 5s 19ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.6137 - val_accuracy: 0.8958\n",
            "Epoch 35/50\n",
            "253/253 [==============================] - 5s 19ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.7093 - val_accuracy: 0.8949\n",
            "Epoch 36/50\n",
            "253/253 [==============================] - 11s 46ms/step - loss: 0.0058 - accuracy: 0.9986 - val_loss: 0.4797 - val_accuracy: 0.8963\n",
            "Epoch 37/50\n",
            "253/253 [==============================] - 13s 51ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.7170 - val_accuracy: 0.8960\n",
            "Epoch 38/50\n",
            "253/253 [==============================] - 13s 51ms/step - loss: 0.0080 - accuracy: 0.9978 - val_loss: 0.6154 - val_accuracy: 0.8958\n",
            "Epoch 39/50\n",
            "253/253 [==============================] - 12s 47ms/step - loss: 0.0044 - accuracy: 0.9991 - val_loss: 0.6723 - val_accuracy: 0.8937\n",
            "Epoch 40/50\n",
            "253/253 [==============================] - 13s 50ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.5513 - val_accuracy: 0.8972\n",
            "Epoch 41/50\n",
            "253/253 [==============================] - 13s 50ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.7484 - val_accuracy: 0.8926\n",
            "Epoch 42/50\n",
            "253/253 [==============================] - 12s 49ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.6760 - val_accuracy: 0.8969\n",
            "Epoch 43/50\n",
            "253/253 [==============================] - 12s 49ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.7797 - val_accuracy: 0.8958\n",
            "Epoch 44/50\n",
            "253/253 [==============================] - 12s 48ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.7961 - val_accuracy: 0.9007\n",
            "Epoch 45/50\n",
            "253/253 [==============================] - 13s 50ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.8050 - val_accuracy: 0.8998\n",
            "Epoch 46/50\n",
            "253/253 [==============================] - 12s 49ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.7696 - val_accuracy: 0.8978\n",
            "Epoch 47/50\n",
            "253/253 [==============================] - 12s 47ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.8013 - val_accuracy: 0.8981\n",
            "Epoch 48/50\n",
            "253/253 [==============================] - 12s 47ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.8156 - val_accuracy: 0.8969\n",
            "Epoch 49/50\n",
            "253/253 [==============================] - 12s 49ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.8213 - val_accuracy: 0.8972\n",
            "Epoch 50/50\n",
            "253/253 [==============================] - 13s 52ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.8165 - val_accuracy: 0.9007\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dc89ea9-5ece-44ea-80bd-2ce4157a0da3",
      "metadata": {
        "tags": [],
        "id": "9dc89ea9-5ece-44ea-80bd-2ce4157a0da3"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "epochs = range(1, len(val_loss_values) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "157d56ca-3c44-42a9-ada9-c02a3d484c38",
      "metadata": {
        "tags": [],
        "id": "157d56ca-3c44-42a9-ada9-c02a3d484c38",
        "outputId": "bbcc5a5e-b6e1-49ab-e345-d5c4707f9e61"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.subplot(211)\n",
        "plt.plot(epochs, history.history['loss'], 'bo', label='Training loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8ae7e0e-8999-49d4-9f62-084612262504",
      "metadata": {
        "tags": [],
        "id": "c8ae7e0e-8999-49d4-9f62-084612262504",
        "outputId": "4c65d891-0e9a-4b62-be75-35ea5737c321"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.subplot(212)\n",
        "plt.plot(epochs, history.history['accuracy'], 'bo', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.gca().set_yticklabels(['{:.0f}%'.format(x * 100) for x in plt.gca().get_yticks()])\n",
        "plt.legend()\n",
        "plt.grid('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81ec1114-2b17-48dc-a357-5c46673cc87c",
      "metadata": {
        "id": "81ec1114-2b17-48dc-a357-5c46673cc87c"
      },
      "source": [
        "## **Anecdotes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38cceeeb-b31a-41ed-ae72-b221d7d7d834",
      "metadata": {
        "tags": [],
        "id": "38cceeeb-b31a-41ed-ae72-b221d7d7d834"
      },
      "outputs": [],
      "source": [
        "def get_sentiment(model, text) -> str:\n",
        "    text_int_embedding = text_to_int(text, word_to_int)\n",
        "    text_int_embedding = pad_sequences(maxlen = sequence_length,\n",
        "                                       sequences = [text_int_embedding],\n",
        "                                       padding = \"post\", value = 0)\n",
        "    sentiment_index = np.argmax(model.predict(text_int_embedding))\n",
        "    return sentiment_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "729bb8a0-a6dd-4371-9727-997f9c50458e",
      "metadata": {
        "tags": [],
        "id": "729bb8a0-a6dd-4371-9727-997f9c50458e",
        "outputId": "10be3527-e94f-4758-af14-eb14dab62c6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 1s 5ms/step\n"
          ]
        }
      ],
      "source": [
        "result = np.argmax(model.predict(X_test), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "710bcbfc-7528-4d7a-bc35-ca71913ee41a",
      "metadata": {
        "tags": [],
        "id": "710bcbfc-7528-4d7a-bc35-ca71913ee41a"
      },
      "outputs": [],
      "source": [
        "positive_sentences = [int_to_text(embedding, int_to_word) for i, embedding in enumerate(X_test) if result[i] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdef917a-c279-4d57-b11f-058560bf7f2d",
      "metadata": {
        "tags": [],
        "id": "fdef917a-c279-4d57-b11f-058560bf7f2d"
      },
      "outputs": [],
      "source": [
        "negative_sentences = [int_to_text(embedding, int_to_word) for i, embedding in enumerate(X_test) if result[i] == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cec25941-c298-469a-a099-79dfdb9b7e0d",
      "metadata": {
        "tags": [],
        "id": "cec25941-c298-469a-a099-79dfdb9b7e0d",
        "outputId": "5f1623e3-b70b-4e08-c8c7-1fea479a83fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['jetblue flight flight booking problems experience pretty great',\n",
              " 'southwestair leeannhealey yeah sale fares got places fly oh damn right live swa fly',\n",
              " 'jetblue news gate options',\n",
              " 'jetblue utah think thanks',\n",
              " 'united flight new york love quality planes united wtf crappy aviation newyork http co zv6cfpohl5']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "positive_sentences[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6597156-a767-4065-8e23-c26d12d71590",
      "metadata": {
        "tags": [],
        "id": "c6597156-a767-4065-8e23-c26d12d71590",
        "outputId": "3515d617-6591-43fe-cb0e-f8862854f647"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['usairways glad airline going swallowed american american always picks phone solves problems',\n",
              " 'southwestair disconnected call 2 5 hours without even speaking octaviannightmare',\n",
              " 'americanair well done taken fun air travel phlairport',\n",
              " 'usairways told coded upgrade clearly purchased seat miles refuse downgrade ripoff',\n",
              " 'rt virginamerica met match got status another airline upgrade restr http co rhkamx9vf5 http co pyalebgkjt']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "negative_sentences[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef70e236-27b3-4947-aaa1-7645ddb3a16b",
      "metadata": {
        "id": "ef70e236-27b3-4947-aaa1-7645ddb3a16b"
      },
      "source": [
        "## **Generating Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "783b77cd-ea53-4e2c-911e-f463b2b52f45",
      "metadata": {
        "id": "783b77cd-ea53-4e2c-911e-f463b2b52f45"
      },
      "source": [
        "The strategy that we'll adopt to generate text is as follows:\n",
        "\n",
        "1. Import project Gutenberg's Alice's Adventures in Wonderland dataset, which can be downloaded from https://www.gutenberg.org/files/11/11-0.txt.\n",
        "2. Preprocess the text data so that we bring every word to the same case, and remove punctuation.\n",
        "3. Assign an ID to each unique word and then convert the dataset into a sequence of word IDs.\n",
        "4. Loop through the total dataset, 10 words at a time. Consider the 10 words as input and the subsequent 11th word as output.\n",
        "5. Build and train a model, by performing embedding on top of the input word IDs and then connecting the embeddings to an LSTM, which is connected to the output layer through a hidden layer. The value in the output layer is the one-hot-encoded version of the output.\n",
        "6. Make a prediction for the subsequent word by taking a random location of word and consider the historical words prior to the location of the random word chosen.\n",
        "7. Move the window of the input words by one from the seed word's location that we chose earlier and the tenth time step word shall be the word that we predicted in the previous step"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c439fb39",
      "metadata": {
        "id": "c439fb39"
      },
      "source": [
        "**Overview**\n",
        "\n",
        "Input Preparation:\n",
        "\n",
        "    The input text is preprocessed by converting it to lowercase and removing any non-alphanumeric characters.\n",
        "    The text is then split into individual words and stored in a list.\n",
        "\n",
        "Vocabulary Building:\n",
        "\n",
        "    The vocabulary is constructed by counting the occurrences of each unique word in the text.\n",
        "    Words are sorted based on their frequency, with more frequent words having higher priority.\n",
        "    Each word is assigned a unique index, creating a mapping between words and their respective indices.\n",
        "\n",
        "Sequence Generation:\n",
        "\n",
        "    To train the model, we need input-output pairs in the form of sequences.\n",
        "    A sliding window approach is employed, where a fixed-length sequence of words (input sequence) is paired with the subsequent word (output word).\n",
        "    By sliding the window over the text with a specified step size, multiple input-output pairs are generated.\n",
        "\n",
        "Encoding the Data:\n",
        "\n",
        "    To feed the data into the LSTM model, we need to encode the input and output sequences into a suitable format.\n",
        "    One-hot encoding is applied to represent each word as a binary vector, indicating whether or not a specific word is present in the sequence.\n",
        "    The input data is converted into a 3D array, where each element represents the presence or absence of a word at a specific position in the sequence.\n",
        "    The output data is converted into a 2D array, where each row corresponds to the one-hot encoded representation of the output word.\n",
        "\n",
        "Model Architecture:\n",
        "\n",
        "    The LSTM model is constructed with a many-to-one architecture.\n",
        "    The input layer receives the encoded input sequences.\n",
        "    The LSTM layer processes the input sequences, capturing the temporal dependencies and learning the patterns in the data.\n",
        "    The LSTM layer outputs a fixed-size representation (hidden state) of the input sequence.\n",
        "    The hidden state is passed through a dense (fully connected) layer with a softmax activation function.\n",
        "    The softmax layer generates a probability distribution over the vocabulary, indicating the likelihood of each word being the next word in the sequence.\n",
        "\n",
        "Training the Model:\n",
        "\n",
        "    The model is trained using the input-output pairs generated from the text data.\n",
        "    During training, the model learns to minimize the difference between the predicted output and the actual output (one-hot encoded representation of the next word).\n",
        "    The model's parameters are updated iteratively using an optimizer (in this case, Adam) and a loss function (categorical cross-entropy).\n",
        "\n",
        "Generating Text:\n",
        "\n",
        "    To generate text, we provide a seed sequence as input to the trained model.\n",
        "    The model predicts the next word in the sequence based on the input.\n",
        "    The predicted word is appended to the input sequence, and the process is repeated to generate subsequent words.\n",
        "    By iteratively predicting the next word, we can generate text that follows the patterns and style learned during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97dfabac-8153-47e1-8e63-5f5fb864d5e7",
      "metadata": {
        "tags": [],
        "id": "97dfabac-8153-47e1-8e63-5f5fb864d5e7"
      },
      "outputs": [],
      "source": [
        "def load_data(filename: str = '../data/alice.txt'):\n",
        "    with open(filename, encoding='utf-8-sig') as fin:\n",
        "        lines = []\n",
        "        for line in fin:\n",
        "            line = line.strip().lower()\n",
        "            if (len(line) == 0):\n",
        "                continue\n",
        "            lines.append(line)\n",
        "        fin.close()\n",
        "        text = \" \".join(lines)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "924ea3e5-666e-4cc5-bd3a-f54d52ab5a34",
      "metadata": {
        "tags": [],
        "id": "924ea3e5-666e-4cc5-bd3a-f54d52ab5a34"
      },
      "outputs": [],
      "source": [
        "text = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f78a57f8-779c-4b53-b0d9-78961433798e",
      "metadata": {
        "tags": [],
        "id": "f78a57f8-779c-4b53-b0d9-78961433798e",
        "outputId": "e706f635-ea19-4e8d-a058-2d826d137ed1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'moment to think about stopping herself before she found herself falling down a very deep well. either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. first, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps '"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[3001:3500]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca9468a7-6e27-40db-9a5f-257354c5567e",
      "metadata": {
        "id": "ca9468a7-6e27-40db-9a5f-257354c5567e"
      },
      "source": [
        "Normalize the text to remove punctuations and convert it to lowercase\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aaefc7c-1473-4f1b-b680-1598c9333368",
      "metadata": {
        "tags": [],
        "id": "3aaefc7c-1473-4f1b-b680-1598c9333368"
      },
      "outputs": [],
      "source": [
        "def pre_process(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6f81900-f446-4f46-8a72-97184ac0eaab",
      "metadata": {
        "id": "e6f81900-f446-4f46-8a72-97184ac0eaab"
      },
      "source": [
        "Assign the unique words to an index so that they can be referenced when constructing the training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4925b6b-e392-4da8-9462-2814fa094d6c",
      "metadata": {
        "tags": [],
        "id": "f4925b6b-e392-4da8-9462-2814fa094d6c"
      },
      "outputs": [],
      "source": [
        "counts = Counter()\n",
        "counts.update(text.split())\n",
        "\n",
        "words = sorted(counts, key=counts.get, reverse=True)\n",
        "nb_words = len(text.split())\n",
        "\n",
        "word2index = {word: i for i, word in enumerate(words)}\n",
        "index2word = {i: word for i, word in enumerate(words)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bb195e3-9e8a-43a8-844a-cd08fb4c081c",
      "metadata": {
        "id": "8bb195e3-9e8a-43a8-844a-cd08fb4c081c"
      },
      "source": [
        "### Preparing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b26eadbf-2f36-490f-91d0-124c697623c3",
      "metadata": {
        "tags": [],
        "id": "b26eadbf-2f36-490f-91d0-124c697623c3"
      },
      "outputs": [],
      "source": [
        "SEQLEN = 10\n",
        "STEP = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97dfe608-000a-46a4-b5cb-e84608757b4f",
      "metadata": {
        "id": "97dfe608-000a-46a4-b5cb-e84608757b4f"
      },
      "source": [
        "Construct the input set of words that leads to an output word. Note that we are considering a sequence of 10 words and trying to predict the 11th word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f1a0752-1e80-4c00-bd24-311606840604",
      "metadata": {
        "tags": [],
        "id": "2f1a0752-1e80-4c00-bd24-311606840604"
      },
      "outputs": [],
      "source": [
        "def get_input_and_labels(text: str,\n",
        "                         seq_length: int = SEQLEN,\n",
        "                         step: int = STEP):\n",
        "\n",
        "    input_words = []\n",
        "    label_words = []\n",
        "\n",
        "    text_arr = text.split()\n",
        "\n",
        "    for i in range(0, nb_words-seq_length, step):\n",
        "        x = text_arr[i:(i+seq_length)]\n",
        "        y = text_arr[i+seq_length]\n",
        "        input_words.append(x)\n",
        "        label_words.append(y)\n",
        "\n",
        "    return input_words, label_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c6371cc-71c4-4052-84b0-5c7febd4be19",
      "metadata": {
        "tags": [],
        "id": "8c6371cc-71c4-4052-84b0-5c7febd4be19"
      },
      "outputs": [],
      "source": [
        "input_words, label_words = get_input_and_labels(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd4eca8f-3228-4d58-accd-aed4dfc810e9",
      "metadata": {
        "tags": [],
        "id": "dd4eca8f-3228-4d58-accd-aed4dfc810e9",
        "outputId": "45d40a9d-693f-446c-db6b-9390ff64cb97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: the project gutenberg ebook of alice’s adventures in wonderland, by\n",
            "Output: lewis\n"
          ]
        }
      ],
      "source": [
        "print(f'Input: {\" \".join(input_words[0])}\\nOutput: {label_words[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6337ca2-b447-4159-8482-b7bd536db594",
      "metadata": {
        "id": "a6337ca2-b447-4159-8482-b7bd536db594"
      },
      "source": [
        "Construct the vectors of the input and the output datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8c2c532-534e-47ce-988d-8eabb7beaa1e",
      "metadata": {
        "tags": [],
        "id": "e8c2c532-534e-47ce-988d-8eabb7beaa1e"
      },
      "outputs": [],
      "source": [
        "total_words = len(set(words))\n",
        "\n",
        "X = np.zeros((len(input_words), SEQLEN, total_words), dtype= bool)\n",
        "y = np.zeros((len(input_words), total_words), dtype=bool)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28507c81-3a88-4c1b-ac83-b52a01a1d91b",
      "metadata": {
        "id": "28507c81-3a88-4c1b-ac83-b52a01a1d91b"
      },
      "source": [
        "We are creating empty arrays in the preceding step, which will be populated in the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70ca2513-711b-42da-b337-f3a0b3633dc9",
      "metadata": {
        "tags": [],
        "id": "70ca2513-711b-42da-b337-f3a0b3633dc9"
      },
      "outputs": [],
      "source": [
        "# Create encoded vectors for the input and output values\n",
        "for i, input_word in enumerate(input_words):\n",
        "    for j, word in enumerate(input_word):\n",
        "        X[i, j, word2index[word]] = 1\n",
        "        y[i, word2index[label_words[i]]] = 1\n",
        "checkpoint('5b420a')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cc5186a-1f98-4970-9bff-7a859332dc70",
      "metadata": {
        "id": "0cc5186a-1f98-4970-9bff-7a859332dc70"
      },
      "source": [
        "In the preceding code, the first for loop is used to loop through all the words in the input sequence of words (10 words in input), and the second for loop is used to loop through an individual word in the chosen sequence of input words. Additionally, given that the output is a list, we do not need to update it using the second for loop (as there is no sequence of IDs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bb4c415-5f03-41b3-b769-a326c8151984",
      "metadata": {
        "tags": [],
        "id": "2bb4c415-5f03-41b3-b769-a326c8151984",
        "outputId": "3be7fb5b-26e8-4922-ece9-1ad3e40f7288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input of X: (29584, 10, 5649)\n",
            "Input of y: (29584, 5649)\n"
          ]
        }
      ],
      "source": [
        "print(f'Input of X: {X.shape}\\nInput of y: {y.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7fd11a2-e619-4d2a-bb49-b737ada7956c",
      "metadata": {
        "id": "d7fd11a2-e619-4d2a-bb49-b737ada7956c"
      },
      "source": [
        "### Defining the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0280a03-5b7d-4a93-ba4d-d0ec2c6aee2d",
      "metadata": {
        "tags": [],
        "id": "e0280a03-5b7d-4a93-ba4d-d0ec2c6aee2d"
      },
      "outputs": [],
      "source": [
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 32\n",
        "NUM_ITERATIONS = 100\n",
        "NUM_EPOCHS_PER_ITERATION = 1\n",
        "NUM_PREDS_PER_EPOCH = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "016337a9-2f6f-4a91-bf70-c2104887e777",
      "metadata": {
        "tags": [],
        "id": "016337a9-2f6f-4a91-bf70-c2104887e777",
        "outputId": "7275552f-2d0a-40a3-e40e-69f2d4ae501a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 128)               2958336   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5649)              728721    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,687,057\n",
            "Trainable params: 3,687,057\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(HIDDEN_SIZE, return_sequences=False, input_shape=(SEQLEN, total_words)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d175afbf-5fd7-4683-9908-f3f1c84f87ff",
      "metadata": {
        "tags": [],
        "id": "d175afbf-5fd7-4683-9908-f3f1c84f87ff"
      },
      "source": [
        "**Fit the model**.\n",
        "- Look at how the output varies over an increasing number of epochs.\n",
        "- Generate a random set of sequences of 10 words and try to predict the next possible word.\n",
        "- We are in a position to observe how our predictions are getting better over an increasing number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff9b2d20-7abb-4ff7-8b5a-182544a11cc5",
      "metadata": {
        "tags": [],
        "id": "ff9b2d20-7abb-4ff7-8b5a-182544a11cc5"
      },
      "outputs": [],
      "source": [
        "def check_model_output(model, preds: int, input_words, seq_length, total_words):\n",
        "    test_idx = np.random.randint(int(len(input_words)*0.1)) * (-1)\n",
        "    test_words = input_words[test_idx]\n",
        "\n",
        "    for curr_pred in range(preds):\n",
        "        curr_embedding = np.zeros((1, seq_length, total_words))\n",
        "\n",
        "        for i, ch in enumerate(test_words):\n",
        "            curr_embedding[0, i, word2index[ch]] = 1\n",
        "\n",
        "        pred = model.predict(curr_embedding, verbose=0)[0]\n",
        "        word_pred = index2word[np.argmax(pred)]\n",
        "\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Prediction {curr_pred + 1} of {preds}\")\n",
        "        print(f'Generating from seed: {\" \".join(test_words)}\\nNext Word: {word_pred}')\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        test_words = test_words[1:] + [word_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c14f7545-afb1-412a-ae50-25c5b92b3daa",
      "metadata": {
        "tags": [],
        "id": "c14f7545-afb1-412a-ae50-25c5b92b3daa"
      },
      "outputs": [],
      "source": [
        "for iteration in range(50):\n",
        "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION, validation_split = 0.1)\n",
        "    if iteration % 10 == 0:\n",
        "        check_model_output(model, 5, input_words, SEQLEN, total_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef015f03-b7a0-4f6c-9f19-67ec752e3ba6",
      "metadata": {
        "tags": [],
        "id": "ef015f03-b7a0-4f6c-9f19-67ec752e3ba6"
      },
      "outputs": [],
      "source": [
        "def predict_next_word(model, input_text: str, seq_length, total_words, temperature = None):\n",
        "    curr_embedding = np.zeros((1, seq_length, total_words))\n",
        "\n",
        "    for i, ch in enumerate(input_text):\n",
        "        curr_embedding[0, i, word2index[ch]] = 1\n",
        "\n",
        "    pred = model.predict(curr_embedding, verbose=0)[0]\n",
        "\n",
        "    if temperature == None:\n",
        "        word_pred = index2word[np.argmax(pred)]\n",
        "    else:\n",
        "        next_word_token = tf.random.categorical(tf.expand_dims(pred / temperature, 0), num_samples=1)[-1, 0].numpy()\n",
        "        word_pred = index2word[next_word_token]\n",
        "\n",
        "    return pred, word_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7609074f-d056-4020-b47a-c41d5986cd71",
      "metadata": {
        "id": "7609074f-d056-4020-b47a-c41d5986cd71"
      },
      "source": [
        "## **The Problem with Text Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2c8d90d-ea12-4cda-ac8e-33539f6c119f",
      "metadata": {
        "id": "c2c8d90d-ea12-4cda-ac8e-33539f6c119f"
      },
      "source": [
        "Language is not always the same and/or stationary, the next word to predict depends on the context, style and etc. Therefore, natural language, uses a wide variety of words, sometimes, with same meaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "378e769a-174d-4fad-86f9-51823a26104c",
      "metadata": {
        "id": "378e769a-174d-4fad-86f9-51823a26104c"
      },
      "source": [
        "Lets have a look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0760f980-a1b9-4f78-898c-8dee56a291ce",
      "metadata": {
        "tags": [],
        "id": "0760f980-a1b9-4f78-898c-8dee56a291ce",
        "outputId": "2ec80770-dd57-4fff-a552-d4dd96ec44b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "among the people that walk with their heads downward! the\n"
          ]
        }
      ],
      "source": [
        "test_words = input_words[-28701]\n",
        "print(' '.join(test_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb6f039-3935-4d8f-a323-ee44a12fdefd",
      "metadata": {
        "tags": [],
        "id": "0eb6f039-3935-4d8f-a323-ee44a12fdefd"
      },
      "outputs": [],
      "source": [
        "logits, word_pred = predict_next_word(model, test_words, SEQLEN, total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02743517-f9e0-42dd-a01b-b0dbc6fcf993",
      "metadata": {
        "tags": [],
        "id": "02743517-f9e0-42dd-a01b-b0dbc6fcf993",
        "outputId": "f7a3f78a-4b70-445a-8478-132d9e8617cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted word: time\n"
          ]
        }
      ],
      "source": [
        "print(f'Predicted word: {word_pred}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "212b2e39-0f0b-44ee-9820-0d6aa42374bb",
      "metadata": {
        "tags": [],
        "id": "212b2e39-0f0b-44ee-9820-0d6aa42374bb"
      },
      "outputs": [],
      "source": [
        "def generate_paragraph(model, seed, words: int, temperature: int):\n",
        "    full_text = seed.copy()\n",
        "    for _ in range(words):\n",
        "        logits, word_pred = predict_next_word(model, seed, SEQLEN, total_words, temperature=temperature)\n",
        "        seed = (seed + [word_pred])[-10:]\n",
        "        full_text = full_text + [word_pred]\n",
        "    return full_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5558a777-a0d6-4390-9211-835c93f48497",
      "metadata": {
        "tags": [],
        "id": "5558a777-a0d6-4390-9211-835c93f48497"
      },
      "outputs": [],
      "source": [
        "for _ in range(5):\n",
        "    print(' '.join(generate_paragraph(model, test_words, 12, None)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c62f72c5-1302-4488-911c-aec3c23602c0",
      "metadata": {
        "id": "c62f72c5-1302-4488-911c-aec3c23602c0"
      },
      "source": [
        "## **Randomness through Entropy Scaling**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e10933f-1b52-42d4-bd95-15e9fb3ed99e",
      "metadata": {
        "id": "7e10933f-1b52-42d4-bd95-15e9fb3ed99e"
      },
      "source": [
        "### What is Entropy in Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2bf013c-1564-4fc8-97c0-248e8e736769",
      "metadata": {
        "id": "f2bf013c-1564-4fc8-97c0-248e8e736769"
      },
      "source": [
        "In Machine Learning, entropy is a measure of the level of disorder or uncertainty in a given dataset or system. It is a metric that quantifies the amount of information in a dataset, and it is commonly used to evaluate the quality of a model and its ability to make accurate predictions. Entropy is based on the concept of probability and is calculated using the formula -sum(p*log2(p)), where p is the probability of each possible outcome. In decision trees, entropy is used to determine the best split at each node and improve the overall accuracy of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d16513-6b05-4d4c-a5a6-522d1d032ec1",
      "metadata": {
        "tags": [],
        "id": "83d16513-6b05-4d4c-a5a6-522d1d032ec1"
      },
      "source": [
        "<img src=\"./images/entropy.png\"\n",
        "     align=\"center\"\n",
        "     width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17f0ca90-38fd-48a4-a1aa-8c781992c2ea",
      "metadata": {
        "id": "17f0ca90-38fd-48a4-a1aa-8c781992c2ea"
      },
      "source": [
        "We can understand the term entropy with any simple example: flipping a coin. When we flip a coin, then there can be two outcomes. However, it is difficult to conclude what would be the exact outcome while flipping a coin because there is no direct relation between flipping a coin and its outcomes. There is a 50% probability of both outcomes; then, in such scenarios, entropy would be high. This is the essence of entropy in machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50b3406d-a4f6-4d93-b95a-5c1dfaf0c16d",
      "metadata": {
        "tags": [],
        "id": "50b3406d-a4f6-4d93-b95a-5c1dfaf0c16d",
        "outputId": "f98db66a-fdd2-4b54-f06e-3406189db352"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.06757716, 0.0514937, 0.02583243, 0.023227636, 0.014906152]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted(logits, reverse=True)[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50c079d1-b570-4cfc-b941-753afd9ea5cc",
      "metadata": {
        "id": "50c079d1-b570-4cfc-b941-753afd9ea5cc"
      },
      "source": [
        "## Softmax Temperature"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74382afb-4ef7-4809-930b-c9fb598381fd",
      "metadata": {
        "id": "74382afb-4ef7-4809-930b-c9fb598381fd"
      },
      "source": [
        "Temperature is a hyperparameter of LSTMs (and neural networks generally) used to control the randomness of predictions by scaling the logits before applying softmax. Temperature scaling has been widely used to improve performance for NLP tasks that utilize the Softmax decision layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3149852-bd26-4f9e-8675-f4c1591cd072",
      "metadata": {
        "tags": [],
        "id": "c3149852-bd26-4f9e-8675-f4c1591cd072"
      },
      "source": [
        "<img src=\"./images/example_softmax_1.png\"\n",
        "     align=\"center\"\n",
        "     width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df8212b1-7a52-420f-980b-b44a13893925",
      "metadata": {
        "id": "df8212b1-7a52-420f-980b-b44a13893925"
      },
      "source": [
        "The generated sequence will have a predictable and generic structure. And the reason is less entropy or randomness in the softmax distribution, in the sense that the likelihood of a particular word (corresponding to index 9 in the above example) getting chosen is way higher than the other words. A sequence being predictable is not problematic as long as the aim is to get realistic sequences. But if the goal is to generate a novel text or an image which has never been seen before, randomness is the holy grail."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fdba9ae-d1c1-4be4-bae1-db66399065e5",
      "metadata": {
        "tags": [],
        "id": "9fdba9ae-d1c1-4be4-bae1-db66399065e5"
      },
      "source": [
        "<img src=\"./images/temp_vs_no_temp.png\"\n",
        "     align=\"center\"\n",
        "     width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c47738d-5e49-4493-aced-326d5ae19eea",
      "metadata": {
        "id": "9c47738d-5e49-4493-aced-326d5ae19eea"
      },
      "source": [
        "<img src=\"./images/temp_animation.gif\"\n",
        "     align=\"center\"\n",
        "     width=\"400\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de38d35f-2d9c-437c-895d-2b57905dd04c",
      "metadata": {
        "id": "de38d35f-2d9c-437c-895d-2b57905dd04c"
      },
      "source": [
        "The distribution above approaches uniform distribution giving each word an equal probability of getting sampled out, thereby rendering a more creative look to the generated sequence. Too much creativity isn’t good either. In the extreme case, the generated text might not make sense at all. Hence, like all other hyperparameters, this needs to be tuned as well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "006ac396-9d3a-46ca-a312-488d5773ef7d",
      "metadata": {
        "id": "006ac396-9d3a-46ca-a312-488d5773ef7d"
      },
      "source": [
        "## Predicting using the Temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1db4279-8aad-48ec-a4ac-a4177a2b8a6c",
      "metadata": {
        "tags": [],
        "id": "e1db4279-8aad-48ec-a4ac-a4177a2b8a6c",
        "outputId": "c0114733-db45-4e1b-92dd-c120be9f1244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "among the people that walk with their heads downward! the alone court,” to.” pop sits bursting particularly “just conversation. pie—” fairly,” pressing\n",
            "among the people that walk with their heads downward! the i. stole gloves, talk. non-profit trumpet rabbit! pink cartwheels, saucepan ran, forepaws\n",
            "among the people that walk with their heads downward! the hearts, them—“i professor yet invited,” states, room ridiculous (or takes “at hopeful\n",
            "among the people that walk with their heads downward! the pool complying reaching tittered he’ll guard bowed baby; nibbling flat chimneys then—i\n",
            "among the people that walk with their heads downward! the off. broken. by: hearts, doth slates, timidly life! offended!” _proves_ porpoise.” mallets\n"
          ]
        }
      ],
      "source": [
        "for _ in range(5):\n",
        "    print(' '.join(generate_paragraph(model, test_words, 12, 10)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94b82846",
      "metadata": {
        "id": "94b82846"
      },
      "source": [
        "## **Conclusion**\n",
        "\n",
        "In this project, we embarked on an exciting journey into sentiment analysis and text generation using many-to-one LSTMs. We began by diving into sentiment detection, training an LSTM model to analyze airline sentiments based on text reviews. Through data preprocessing, bag-of-words representation, and the many-to-one LSTM architecture, we successfully predicted sentiment labels with high accuracy.\n",
        "\n",
        "Moving on to text generation, we leveraged the iconic literary work \"Alice's Adventures in Wonderland\" to train many-to-one LSTMs to generate contextually relevant text. We tackled the challenges of text generation, including language variability and word choice, and employed techniques such as entropy scaling and softmax temperature to control randomness and enhance the diversity of the generated text.\n",
        "\n",
        "Throughout the project, we explored various concepts and techniques, from preprocessing textual data to training and evaluating LSTM models. We gained insights into sentiment analysis, understanding the significance of analyzing sentiments in textual data. Additionally, we delved into the intricacies of text generation, honing our skills in next-word prediction and generating coherent sentences.\n",
        "\n",
        "As you continue your journey, you can build upon this project by exploring different datasets, experimenting with hyperparameters, and expanding the scope of sentiment analysis and text generation. Deepening your understanding of LSTMs and exploring advanced techniques like attention mechanisms can further enhance the quality and sophistication of your models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d474e81",
      "metadata": {
        "id": "4d474e81"
      },
      "source": [
        "## **Interview Questions**\n",
        "\n",
        "* Explain the concept of a bag-of-words representation and its role in sentiment analysis.\n",
        "\n",
        "* How does the preprocessing of textual data help in improving sentiment analysis results?\n",
        "\n",
        "* Describe the steps involved in training an LSTM model for sentiment analysis.\n",
        "* What is the purpose of the softmax activation function in the output layer of the LSTM model?\n",
        "* How do you evaluate the performance of an LSTM model in sentiment analysis?\n",
        "* Explain the challenges associated with text generation using many-to-one LSTMs.\n",
        "\n",
        "\n",
        "* What role does data preprocessing play in text generation using many-to-one LSTMs?\n",
        "\n",
        "* Describe the steps involved in generating new text using a trained LSTM model.\n",
        "\n",
        "* What is the purpose of entropy scaling in text generation? How does it impact the generated text?\n",
        "\n",
        "* Explain the concept of softmax temperature and its role in controlling prediction randomness.\n",
        "\n",
        "* How can you ensure that the generated text is contextually relevant and coherent?\n",
        "\n",
        "* What techniques can be used to improve the diversity and creativity of the generated text?\n",
        "\n",
        "* Discuss the trade-off between randomness and coherence in text generation using LSTMs.\n",
        "\n",
        "* How can you handle out-of-vocabulary words when training an LSTM model for text generation?\n",
        "\n",
        "\n",
        "* What are the limitations of using many-to-one LSTMs for text generation?\n",
        "\n",
        "* How can you handle long-term dependencies in text generation using LSTMs?\n",
        "\n",
        "* Discuss the impact of different hyperparameters on the performance of LSTM models in text generation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a589993",
      "metadata": {
        "id": "9a589993"
      },
      "source": [
        "## **Feedback**\n",
        "\n",
        "We will appreciate your comments on the quality of the material we supplied. Kindly post your comments.\n",
        "\n",
        "\n",
        "If you want to see any specific project, post it [here](https://www.projectpro.io/project/upcoming-projects).\n",
        "\n",
        "Click [here](https://www.projectpro.io/contact-us) to get in touch with us."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f078d8",
      "metadata": {
        "id": "94f078d8"
      },
      "source": [
        "\n",
        "<h1><center>Thank you for choosing ProjectPro!</center></h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "306012ae-f900-4e28-963c-12cd527b9ef7",
      "metadata": {
        "id": "306012ae-f900-4e28-963c-12cd527b9ef7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}